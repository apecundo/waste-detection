{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pecundo, Allan Magno E.\n",
    "# CSCI 214-A\n",
    "# Project Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create function to prepare dataset needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = 'Garbage Classification Kaggle/Special Training Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "folder = 'Garbage Classification Kaggle/Training Augmentations/'\n",
    "\n",
    "def get_data(folder, limit = None):\n",
    "\n",
    "    labels = []\n",
    "    data = []\n",
    "    img = []\n",
    "\n",
    "    for j in os.listdir(folder):\n",
    "\n",
    "\n",
    "        for i in os.listdir(os.path.join(folder,j)):\n",
    "\n",
    "\n",
    "            labels.append(j)\n",
    "\n",
    "            temp_data = cv2.imread(os.path.join(folder,j,i))\n",
    "            temp_data = cv2.resize(temp_data, (200, 200))\n",
    "            img.append(temp_data)\n",
    "\n",
    "            data.append(temp_data)\n",
    "\n",
    "            \n",
    "    data = np.array(data)\n",
    "            \n",
    "            \n",
    "    return img, data, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Creating the Detector - OpenCV Selective Search with CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# import the necessary packages\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, AveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, ImageDataGenerator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation and Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = './Garbage Classification Kaggle/Special Training Data/'\n",
    "\n",
    "img, df, labels = get_data(folder = folder, limit = None)\n",
    "\n",
    "battery_index = [labels == 'battery']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Augmentation from reading images in folders\n",
    "#https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/\n",
    "\n",
    "#### Apply Data Augmentation to Battery Class only\n",
    "\n",
    "filepath = './Garbage Classification Kaggle/Special Training Data/battery'\n",
    "\n",
    "random_state = 42\n",
    "rotation_range = 180\n",
    "width_shift_range = 0.1\n",
    "height_shift_range=0.1\n",
    "brightness_range=None\n",
    "shear_range = 0.2\n",
    "horizontal_flip = True\n",
    "vertical_flip = True\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "\n",
    "datagen = ImageDataGenerator(rotation_range = rotation_range,\n",
    "                         width_shift_range = width_shift_range,\n",
    "                         height_shift_range=height_shift_range,\n",
    "                         shear_range = shear_range,\n",
    "                         horizontal_flip = horizontal_flip,\n",
    "                         vertical_flip = vertical_flip,\n",
    "                        validation_split = 0.2)\n",
    "\n",
    "for f in os.listdir(filepath):\n",
    "    img = load_img(os.path.join(filepath,f))  \n",
    "    x = img_to_array(img) \n",
    "    \n",
    "    # Reshape the input image \n",
    "    x = x.reshape((1, ) + x.shape)  \n",
    "    i = 0\n",
    "\n",
    "    # generate 10 new augmented images \n",
    "    for batch in datagen.flow(x, batch_size = 1, \n",
    "                              save_to_dir = './Garbage Classification Kaggle/Training Augmentations/battery', \n",
    "                              save_format ='jpeg'):\n",
    "        i += 1\n",
    "        if i > 10: \n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['battery', 'not-battery'], dtype='<U11')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(225, 225, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(img).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the augmentations, the training set was reviewed again to see if there were images with large background. This was further cropped, outside of the Jupyter notebook, to see if it can make the classifier choose the better bounding box proposed for the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Classifier Using ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = './Garbage Classification Kaggle/Training Augmentations/Set3' #Folder contains augmented battery image and balanced 'not-battery' class containing some pictures from the other class of the original dataset\n",
    "\n",
    "img, df, labels = get_data(folder = folder, limit = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2409, 200, 200, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing arrays\n",
    "img = np.array(img)\n",
    "df = np.array(df)\n",
    "labels = np.array(labels)\n",
    "\n",
    "#Label encoder\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "oe = OneHotEncoder()\n",
    "\n",
    "labels2 = oe.fit_transform(labels.reshape(-1,1)).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df, labels2, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.pyimagesearch.com/2020/04/27/fine-tuning-resnet-with-keras-tensorflow-and-deep-learning/\n",
    "\n",
    "model_base = ResNet50(include_top = False, input_tensor = Input(shape = (200,200,3)))\n",
    "\n",
    "\n",
    "model_test = model_base.output\n",
    "\n",
    "#Modifying last layer to classify into to classes only: \"battery\", \"not-battery\"\n",
    "\n",
    "model_test = AveragePooling2D(pool_size=(7, 7))(model_test)\n",
    "model_test = Flatten(name=\"flatten\")(model_test)\n",
    "model_test = Dense(256, activation=\"relu\")(model_test)\n",
    "model_test = Dropout(0.5)(model_test)\n",
    "model_test = Dense(2, activation= \"softmax\")(model_test)\n",
    "\n",
    "# place the head FC model on top of the base model (this will become\n",
    "# the actual model we will train)\n",
    "\n",
    "model = Model(inputs = model_base.input, outputs = model_test)\n",
    "# loop over all layers in the base model and freeze them so they will\n",
    "# *not* be updated during the training process\n",
    "for layer in model_base.layers:\n",
    "\tlayer.trainable = False\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer = 'Adam',\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "121/121 [==============================] - 12s 37ms/step - loss: 0.0989 - accuracy: 0.9637 - val_loss: 0.0139 - val_accuracy: 0.9959\n",
      "Epoch 2/10\n",
      "121/121 [==============================] - 3s 28ms/step - loss: 0.0215 - accuracy: 0.9953 - val_loss: 0.0110 - val_accuracy: 0.9979\n",
      "Epoch 3/10\n",
      "121/121 [==============================] - 3s 29ms/step - loss: 0.0097 - accuracy: 0.9964 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "121/121 [==============================] - 3s 29ms/step - loss: 0.0065 - accuracy: 0.9979 - val_loss: 0.0032 - val_accuracy: 0.9979\n",
      "Epoch 5/10\n",
      "121/121 [==============================] - 3s 28ms/step - loss: 0.0076 - accuracy: 0.9984 - val_loss: 0.0070 - val_accuracy: 0.9979\n",
      "Epoch 6/10\n",
      "121/121 [==============================] - 3s 28ms/step - loss: 0.0065 - accuracy: 0.9984 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "121/121 [==============================] - 3s 28ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "121/121 [==============================] - 3s 27ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "121/121 [==============================] - 3s 27ms/step - loss: 4.6266e-04 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "121/121 [==============================] - 3s 28ms/step - loss: 3.1367e-04 - accuracy: 1.0000 - val_loss: 9.1412e-04 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x157001455e0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x = X_train, y = y_train, \n",
    "          batch_size = 16,\n",
    "          epochs = 10,\n",
    "          validation_data = (X_test, y_test),\n",
    "          \n",
    "            \n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\ml\\lib\\site-packages\\keras\\utils\\generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    }
   ],
   "source": [
    "model.save('model_resnet_augmentations4.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detector Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# import the necessary packages\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, AveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.applications import imagenet_utils\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from imutils.object_detection import non_max_suppression\n",
    "import numpy as np\n",
    "import argparse\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model_resnet_augmentations4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selective_search(image, method=\"fast\"):\n",
    "\t# initialize OpenCV's selective search implementation and set the\n",
    "\t# input image\n",
    "\tss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
    "\tss.setBaseImage(image)\n",
    "\t# check to see if we are using the *fast* but *less accurate* version\n",
    "\t# of selective search\n",
    "\tif method == \"fast\":\n",
    "\t\tss.switchToSelectiveSearchFast()\n",
    "\t# otherwise we are using the *slower* but *more accurate* version\n",
    "\telse:\n",
    "\t\tss.switchToSelectiveSearchQuality()\n",
    "\t# run selective search on the input image\n",
    "\trects = ss.process()\n",
    "\t# return the region proposal bounding boxes\n",
    "\treturn rects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Modifying non_max_suppression from imutils to also return probability score from https://github.com/PyImageSearch/imutils/blob/master/imutils/object_detection.py\n",
    "\n",
    "def non_max_suppression_withscore(boxes, probs=None, overlapThresh=0.3):\n",
    "    \n",
    "    import numpy as np\n",
    "    # if there are no boxes, return an empty list\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "\n",
    "    # if the bounding boxes are integers, convert them to floats -- this\n",
    "    # is important since we'll be doing a bunch of divisions\n",
    "    if boxes.dtype.kind == \"i\":\n",
    "        boxes = boxes.astype(\"float\")\n",
    "\n",
    "    # initialize the list of picked indexes\n",
    "    pick = []\n",
    "\n",
    "    # grab the coordinates of the bounding boxes\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    x2 = boxes[:, 2]\n",
    "    y2 = boxes[:, 3]\n",
    "\n",
    "    # compute the area of the bounding boxes and grab the indexes to sort\n",
    "    # (in the case that no probabilities are provided, simply sort on the\n",
    "    # bottom-left y-coordinate)\n",
    "    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    idxs = y2\n",
    "\n",
    "    # if probabilities are provided, sort on them instead\n",
    "    if probs is not None:\n",
    "        idxs = probs\n",
    "\n",
    "    # sort the indexes\n",
    "    idxs = np.argsort(idxs)\n",
    "\n",
    "    # keep looping while some indexes still remain in the indexes list\n",
    "    while len(idxs) > 0:\n",
    "        # grab the last index in the indexes list and add the index value\n",
    "        # to the list of picked indexes\n",
    "        last = len(idxs) - 1\n",
    "        i = idxs[last]\n",
    "        pick.append(i)\n",
    "\n",
    "        # find the largest (x, y) coordinates for the start of the bounding\n",
    "        # box and the smallest (x, y) coordinates for the end of the bounding\n",
    "        # box\n",
    "        xx1 = np.maximum(x1[i], x1[idxs[:last]])\n",
    "        yy1 = np.maximum(y1[i], y1[idxs[:last]])\n",
    "        xx2 = np.minimum(x2[i], x2[idxs[:last]])\n",
    "        yy2 = np.minimum(y2[i], y2[idxs[:last]])\n",
    "\n",
    "        # compute the width and height of the bounding box\n",
    "        w = np.maximum(0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0, yy2 - yy1 + 1)\n",
    "\n",
    "        # compute the ratio of overlap\n",
    "        overlap = (w * h) / area[idxs[:last]]\n",
    "\n",
    "        # delete all indexes from the index list that have overlap greater\n",
    "        # than the provided overlap threshold\n",
    "        idxs = np.delete(idxs, np.concatenate(([last],\n",
    "            np.where(overlap > overlapThresh)[0])))\n",
    "\n",
    "    # return only the bounding boxes that were picked\n",
    "    return boxes[pick].astype(\"int\"), probs[pick].astype('float32')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automating Workflow for Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Code template taken from https://www.pyimagesearch.com/2020/07/06/region-proposal-object-detection-with-opencv-keras-and-tensorflow/\n",
    "\n",
    "def generate_prediction(file, thresh):\n",
    "    \n",
    "    %matplotlib inline\n",
    "\n",
    "    test_img = cv2.imread(file)\n",
    "    \n",
    "    #Decide on window size limit\n",
    "\n",
    "    (H, W) = test_img.shape[:2]\n",
    "\n",
    "    # run selective search on the input image\n",
    "    print(\"[INFO] performing selective search with 'fast' method...\")\n",
    "    rects = selective_search(test_img, method='fast')\n",
    "    print(\"[INFO] {} regions found by selective search\".format(len(rects)))\n",
    "    # initialize the list of region proposals that we'll be classifying\n",
    "    # along with their associated bounding boxes\n",
    "    proposals = []\n",
    "    boxes = []\n",
    "    \n",
    "    #Loop through the different bounding box proposals of Selective Search\n",
    "    for (x, y, w, h) in rects:\n",
    "        \n",
    "        \n",
    "    # if the width or height of the region is less than 10% of the  image width or height, ignore it (i.e., filter out small\n",
    "    # objects that are likely false-positives)\n",
    "    \n",
    "        if w / float(W) < 0.1 or h / float(H) < 0.1:\n",
    "            continue\n",
    "        # extract the region from the input image, convert it from BGR to\n",
    "        # RGB channel ordering, and then resize it to 200 x 200 (the input\n",
    "        # dimensions required by our pre-trained CNN)\n",
    "        \n",
    "        roi = test_img[y:y + h, x:x + w]\n",
    "        roi = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB) #Convert to RGB\n",
    "        #print(roi.shape)\n",
    "        roi = cv2.resize(roi, (200,200))\n",
    "        # further preprocess by the ROI\n",
    "        roi = img_to_array(roi)\n",
    "        roi = preprocess_input(roi)\n",
    "        # update our proposals and bounding boxes lists\n",
    "        proposals.append(roi)\n",
    "        boxes.append((x, y, w, h))\n",
    "\n",
    "    \n",
    "    #List of class labels used for limiting what classes are to be detected.\n",
    "    labelFilters = ['battery','not-battery']\n",
    "    \n",
    "    \n",
    "    proposals = np.array(proposals)\n",
    "    print(\"[INFO] proposal shape: {}\".format(proposals.shape))\n",
    "    # classify each of the proposal ROIs using trained ResNet-50 model\n",
    "    print(\"[INFO] classifying proposals...\")\n",
    "    preds = model.predict(proposals)\n",
    "\n",
    "    # initialize a dictionary which maps class labels (keys) to any\n",
    "    # bounding box associated with that label (values)\n",
    "    labels = {}\n",
    "    confidence = 0.95 #Set confidence filter for detection to limit possible bounding box predictions to ones likely to be actual image.\n",
    "\n",
    "    # loop over the predictions\n",
    "    for (i, p) in enumerate(preds):\n",
    "        \n",
    "        # grab the prediction information for the current region proposal\n",
    "        label = labelFilters[np.argmax(p)] #Get class name of prediction based on highest probability class\n",
    "        \n",
    "        class_id = np.argmax(p)\n",
    "        \n",
    "        prob = p[np.argmax(p)] #Get probability of predictionfor the highest probability class\n",
    "        \n",
    "        # only if the label filters are not empty *and* the label does not\n",
    "        # exist in the list, then ignore it\n",
    "        \n",
    "        \n",
    "        \n",
    "        if labelFilters is not None and label not in labelFilters:\n",
    "            continue\n",
    "        # filter out weak detections by ensuring the predicted probability\n",
    "        # is greater than the minimum probability\n",
    "        if prob >= confidence:\n",
    "            # grab the bounding box associated with the prediction and\n",
    "            # convert the coordinates\n",
    "            (x, y, w, h) = boxes[i]\n",
    "            box = (x, y, x + w, y + h)\n",
    "            # grab the list of predictions for the label and add the\n",
    "            # bounding box + probability to the list\n",
    "            \n",
    "            L = labels.get(label, [])\n",
    "            L.append((box, prob))\n",
    "            labels[label] = L\n",
    "            \n",
    "            \n",
    "            class_label = ['battery'] #Set list of class labels that will be searched/detected in the testing set\n",
    "   \n",
    "    \n",
    "\n",
    "    for label in class_label: #Run detection per class filter\n",
    "        \n",
    "        # clone the original image so that we can draw on it\n",
    "        print(\"[INFO] showing results for '{}'\".format(label))\n",
    "        clone = test_img.copy()\n",
    "        \n",
    "        # loop over all bounding boxes for the current label\n",
    "        for (box, prob) in labels[label]:\n",
    "            # draw the bounding box on the image\n",
    "            (startX, startY, endX, endY) = box\n",
    "            cv2.rectangle(clone, (startX, startY), (endX, endY),\n",
    "                (0, 255, 0), 2)\n",
    "            \n",
    "            \n",
    "\n",
    "            # extract the bounding boxes and associated prediction\n",
    "        # probabilities, then apply non-maxima suppression\n",
    "        boxes = np.array([p[0] for p in labels[label]])\n",
    "        proba = np.array([p[1] for p in labels[label]])\n",
    "        boxes, proba = non_max_suppression_withscore(boxes, proba, overlapThresh = thresh) #Apply non-max suppression to limit number of boxes predicted to most likely.\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "    label_list = []\n",
    "    \n",
    "    \n",
    "    for p in boxes:    \n",
    "        label_list.append(label)\n",
    "\n",
    "    \n",
    "    #Get list of labels predicted in test set\n",
    "    label_list = np.array(label_list)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    labels_df = pd.DataFrame(label_list)\n",
    "\n",
    "    print(labels_df)\n",
    "    \n",
    "    \n",
    "    #Get list of prediction probability/confidence predicted for each final prediction/detection of the class in test set\n",
    "    prob_df = pd.DataFrame(proba)\n",
    "\n",
    "    print(prob_df)\n",
    "    \n",
    "    \n",
    "    #Get list of bounding box or region predicted for each final prediction/detection of the class in test set\n",
    "    \n",
    "    boxes_df = pd.DataFrame(boxes)\n",
    "\n",
    "    print(boxes_df)\n",
    "    \n",
    "    \n",
    "    #Dataframe format of results for mAP performance metric check\n",
    "    \n",
    "    \n",
    "    results = labels_df.merge(prob_df.merge(boxes_df, left_index = True, right_index = True), left_index = True, right_index = True)\n",
    "    #results = labels_df.merge(prob_df.merge(boxes_df[[0,3,2,1]], left_index = True, right_index = True), left_index = True, right_index = True)\n",
    "    \n",
    "    return results\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] performing selective search with 'fast' method...\n",
      "[INFO] 12137 regions found by selective search\n",
      "[INFO] proposal shape: (1807, 200, 200, 3)\n",
      "[INFO] classifying proposals...\n",
      "[INFO] showing results for 'battery'\n",
      "         0\n",
      "0  battery\n",
      "1  battery\n",
      "2  battery\n",
      "3  battery\n",
      "4  battery\n",
      "5  battery\n",
      "          0\n",
      "0  0.999980\n",
      "1  0.999916\n",
      "2  0.998181\n",
      "3  0.995172\n",
      "4  0.987059\n",
      "5  0.982066\n",
      "      0    1     2    3\n",
      "0   812  586  1054  745\n",
      "1    14  718   217  941\n",
      "2     0  540   172  732\n",
      "3  1007  525  1200  741\n",
      "4   296  496   479  637\n",
      "5   725  353  1079  664\n",
      "         0       0_x   0_y    1     2    3\n",
      "0  battery  0.999980   812  586  1054  745\n",
      "1  battery  0.999916    14  718   217  941\n",
      "2  battery  0.998181     0  540   172  732\n",
      "3  battery  0.995172  1007  525  1200  741\n",
      "4  battery  0.987059   296  496   479  637\n",
      "5  battery  0.982066   725  353  1079  664\n"
     ]
    }
   ],
   "source": [
    "file = 'Garbage Classification Kaggle/Testing/images/test_img13.jpg'\n",
    "\n",
    "\n",
    "results = generate_prediction(file, thresh = 0.2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<b> Note: For images where batteries were not detected at all, a blank detection .txt file created and used to signify that no detection was done for the purpose of the mAP measurement. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('./Garbage Classification Kaggle/Testing/detections3/test_img13.txt',sep= '\\t',header = False, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detection results were saved as a .txt file. This workflow (reading image -> generating detections -> saving as .txt file) was done one at a time per test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring the mAP performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mAP was measured using the mAP library by Cartucho (https://github.com/Cartucho/mAP). <b>This was run externally on command prompt on a separate directory. </b> The script used, along with modifications to the IoU threshold are based on the template below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-na] [-np] [-q] [-i IGNORE [IGNORE ...]]\n",
      "                             [--set-class-iou SET_CLASS_IOU [SET_CLASS_IOU ...]]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\User\\AppData\\Roaming\\jupyter\\runtime\\kernel-92785235-a684-40f7-b055-70e3a1520dd6.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\ml\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3452: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import operator\n",
    "import sys\n",
    "import argparse\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "###Note: The MINOVERLAP was modified to adjust the allowable IoU for considering detection.\n",
    "\n",
    "MINOVERLAP = 0.5 # default value (defined in the PASCAL VOC2012 challenge)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-na', '--no-animation', help=\"no animation is shown.\", action=\"store_true\")\n",
    "parser.add_argument('-np', '--no-plot', help=\"no plot is shown.\", action=\"store_true\")\n",
    "parser.add_argument('-q', '--quiet', help=\"minimalistic console output.\", action=\"store_true\")\n",
    "# argparse receiving list of classes to be ignored (e.g., python main.py --ignore person book)\n",
    "parser.add_argument('-i', '--ignore', nargs='+', type=str, help=\"ignore a list of classes.\")\n",
    "# argparse receiving list of classes with specific IoU (e.g., python main.py --set-class-iou person 0.7)\n",
    "parser.add_argument('--set-class-iou', nargs='+', type=str, help=\"set IoU for a specific class.\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "'''\n",
    "    0,0 ------> x (width)\n",
    "     |\n",
    "     |  (Left,Top)\n",
    "     |      *_________\n",
    "     |      |         |\n",
    "            |         |\n",
    "     y      |_________|\n",
    "  (height)            *\n",
    "                (Right,Bottom)\n",
    "'''\n",
    "\n",
    "# if there are no classes to ignore then replace None by empty list\n",
    "if args.ignore is None:\n",
    "    args.ignore = []\n",
    "\n",
    "specific_iou_flagged = False\n",
    "if args.set_class_iou is not None:\n",
    "    specific_iou_flagged = True\n",
    "\n",
    "# make sure that the cwd() is the location of the python script (so that every path makes sense)\n",
    "os.chdir(os.path.dirname(os.path.abspath(__file__)))\n",
    "\n",
    "GT_PATH = os.path.join(os.getcwd(), 'input', 'ground-truth')\n",
    "DR_PATH = os.path.join(os.getcwd(), 'input', 'detection-results')\n",
    "# if there are no images then no animation can be shown\n",
    "IMG_PATH = os.path.join(os.getcwd(), 'input', 'images-optional')\n",
    "if os.path.exists(IMG_PATH): \n",
    "    for dirpath, dirnames, files in os.walk(IMG_PATH):\n",
    "        if not files:\n",
    "            # no image files found\n",
    "            args.no_animation = True\n",
    "else:\n",
    "    args.no_animation = True\n",
    "\n",
    "# try to import OpenCV if the user didn't choose the option --no-animation\n",
    "show_animation = False\n",
    "if not args.no_animation:\n",
    "    try:\n",
    "        import cv2\n",
    "        show_animation = True\n",
    "    except ImportError:\n",
    "        print(\"\\\"opencv-python\\\" not found, please install to visualize the results.\")\n",
    "        args.no_animation = True\n",
    "\n",
    "# try to import Matplotlib if the user didn't choose the option --no-plot\n",
    "draw_plot = False\n",
    "if not args.no_plot:\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        draw_plot = True\n",
    "    except ImportError:\n",
    "        print(\"\\\"matplotlib\\\" not found, please install it to get the resulting plots.\")\n",
    "        args.no_plot = True\n",
    "\n",
    "\n",
    "def log_average_miss_rate(prec, rec, num_images):\n",
    "    \"\"\"\n",
    "        log-average miss rate:\n",
    "            Calculated by averaging miss rates at 9 evenly spaced FPPI points\n",
    "            between 10e-2 and 10e0, in log-space.\n",
    "\n",
    "        output:\n",
    "                lamr | log-average miss rate\n",
    "                mr | miss rate\n",
    "                fppi | false positives per image\n",
    "\n",
    "        references:\n",
    "            [1] Dollar, Piotr, et al. \"Pedestrian Detection: An Evaluation of the\n",
    "               State of the Art.\" Pattern Analysis and Machine Intelligence, IEEE\n",
    "               Transactions on 34.4 (2012): 743 - 761.\n",
    "    \"\"\"\n",
    "\n",
    "    # if there were no detections of that class\n",
    "    if prec.size == 0:\n",
    "        lamr = 0\n",
    "        mr = 1\n",
    "        fppi = 0\n",
    "        return lamr, mr, fppi\n",
    "\n",
    "    fppi = (1 - prec)\n",
    "    mr = (1 - rec)\n",
    "\n",
    "    fppi_tmp = np.insert(fppi, 0, -1.0)\n",
    "    mr_tmp = np.insert(mr, 0, 1.0)\n",
    "\n",
    "    # Use 9 evenly spaced reference points in log-space\n",
    "    ref = np.logspace(-2.0, 0.0, num = 9)\n",
    "    for i, ref_i in enumerate(ref):\n",
    "        # np.where() will always find at least 1 index, since min(ref) = 0.01 and min(fppi_tmp) = -1.0\n",
    "        j = np.where(fppi_tmp <= ref_i)[-1][-1]\n",
    "        ref[i] = mr_tmp[j]\n",
    "\n",
    "    # log(0) is undefined, so we use the np.maximum(1e-10, ref)\n",
    "    lamr = math.exp(np.mean(np.log(np.maximum(1e-10, ref))))\n",
    "\n",
    "    return lamr, mr, fppi\n",
    "\n",
    "\"\"\"\n",
    " throw error and exit\n",
    "\"\"\"\n",
    "def error(msg):\n",
    "    print(msg)\n",
    "    sys.exit(0)\n",
    "\n",
    "\"\"\"\n",
    " check if the number is a float between 0.0 and 1.0\n",
    "\"\"\"\n",
    "def is_float_between_0_and_1(value):\n",
    "    try:\n",
    "        val = float(value)\n",
    "        if val > 0.0 and val < 1.0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\"\"\"\n",
    " Calculate the AP given the recall and precision array\n",
    "    1st) We compute a version of the measured precision/recall curve with\n",
    "         precision monotonically decreasing\n",
    "    2nd) We compute the AP as the area under this curve by numerical integration.\n",
    "\"\"\"\n",
    "def voc_ap(rec, prec):\n",
    "    \"\"\"\n",
    "    --- Official matlab code VOC2012---\n",
    "    mrec=[0 ; rec ; 1];\n",
    "    mpre=[0 ; prec ; 0];\n",
    "    for i=numel(mpre)-1:-1:1\n",
    "            mpre(i)=max(mpre(i),mpre(i+1));\n",
    "    end\n",
    "    i=find(mrec(2:end)~=mrec(1:end-1))+1;\n",
    "    ap=sum((mrec(i)-mrec(i-1)).*mpre(i));\n",
    "    \"\"\"\n",
    "    rec.insert(0, 0.0) # insert 0.0 at begining of list\n",
    "    rec.append(1.0) # insert 1.0 at end of list\n",
    "    mrec = rec[:]\n",
    "    prec.insert(0, 0.0) # insert 0.0 at begining of list\n",
    "    prec.append(0.0) # insert 0.0 at end of list\n",
    "    mpre = prec[:]\n",
    "    \"\"\"\n",
    "     This part makes the precision monotonically decreasing\n",
    "        (goes from the end to the beginning)\n",
    "        matlab: for i=numel(mpre)-1:-1:1\n",
    "                    mpre(i)=max(mpre(i),mpre(i+1));\n",
    "    \"\"\"\n",
    "    # matlab indexes start in 1 but python in 0, so I have to do:\n",
    "    #     range(start=(len(mpre) - 2), end=0, step=-1)\n",
    "    # also the python function range excludes the end, resulting in:\n",
    "    #     range(start=(len(mpre) - 2), end=-1, step=-1)\n",
    "    for i in range(len(mpre)-2, -1, -1):\n",
    "        mpre[i] = max(mpre[i], mpre[i+1])\n",
    "    \"\"\"\n",
    "     This part creates a list of indexes where the recall changes\n",
    "        matlab: i=find(mrec(2:end)~=mrec(1:end-1))+1;\n",
    "    \"\"\"\n",
    "    i_list = []\n",
    "    for i in range(1, len(mrec)):\n",
    "        if mrec[i] != mrec[i-1]:\n",
    "            i_list.append(i) # if it was matlab would be i + 1\n",
    "    \"\"\"\n",
    "     The Average Precision (AP) is the area under the curve\n",
    "        (numerical integration)\n",
    "        matlab: ap=sum((mrec(i)-mrec(i-1)).*mpre(i));\n",
    "    \"\"\"\n",
    "    ap = 0.0\n",
    "    for i in i_list:\n",
    "        ap += ((mrec[i]-mrec[i-1])*mpre[i])\n",
    "    return ap, mrec, mpre\n",
    "\n",
    "\n",
    "\"\"\"\n",
    " Convert the lines of a file to a list\n",
    "\"\"\"\n",
    "def file_lines_to_list(path):\n",
    "    # open txt file lines to a list\n",
    "    with open(path) as f:\n",
    "        content = f.readlines()\n",
    "    # remove whitespace characters like `\\n` at the end of each line\n",
    "    content = [x.strip() for x in content]\n",
    "    return content\n",
    "\n",
    "\"\"\"\n",
    " Draws text in image\n",
    "\"\"\"\n",
    "def draw_text_in_image(img, text, pos, color, line_width):\n",
    "    font = cv2.FONT_HERSHEY_PLAIN\n",
    "    fontScale = 1\n",
    "    lineType = 1\n",
    "    bottomLeftCornerOfText = pos\n",
    "    cv2.putText(img, text,\n",
    "            bottomLeftCornerOfText,\n",
    "            font,\n",
    "            fontScale,\n",
    "            color,\n",
    "            lineType)\n",
    "    text_width, _ = cv2.getTextSize(text, font, fontScale, lineType)[0]\n",
    "    return img, (line_width + text_width)\n",
    "\n",
    "\"\"\"\n",
    " Plot - adjust axes\n",
    "\"\"\"\n",
    "def adjust_axes(r, t, fig, axes):\n",
    "    # get text width for re-scaling\n",
    "    bb = t.get_window_extent(renderer=r)\n",
    "    text_width_inches = bb.width / fig.dpi\n",
    "    # get axis width in inches\n",
    "    current_fig_width = fig.get_figwidth()\n",
    "    new_fig_width = current_fig_width + text_width_inches\n",
    "    propotion = new_fig_width / current_fig_width\n",
    "    # get axis limit\n",
    "    x_lim = axes.get_xlim()\n",
    "    axes.set_xlim([x_lim[0], x_lim[1]*propotion])\n",
    "\n",
    "\"\"\"\n",
    " Draw plot using Matplotlib\n",
    "\"\"\"\n",
    "def draw_plot_func(dictionary, n_classes, window_title, plot_title, x_label, output_path, to_show, plot_color, true_p_bar):\n",
    "    # sort the dictionary by decreasing value, into a list of tuples\n",
    "    sorted_dic_by_value = sorted(dictionary.items(), key=operator.itemgetter(1))\n",
    "    # unpacking the list of tuples into two lists\n",
    "    sorted_keys, sorted_values = zip(*sorted_dic_by_value)\n",
    "    # \n",
    "    if true_p_bar != \"\":\n",
    "        \"\"\"\n",
    "         Special case to draw in:\n",
    "            - green -> TP: True Positives (object detected and matches ground-truth)\n",
    "            - red -> FP: False Positives (object detected but does not match ground-truth)\n",
    "            - pink -> FN: False Negatives (object not detected but present in the ground-truth)\n",
    "        \"\"\"\n",
    "        fp_sorted = []\n",
    "        tp_sorted = []\n",
    "        for key in sorted_keys:\n",
    "            fp_sorted.append(dictionary[key] - true_p_bar[key])\n",
    "            tp_sorted.append(true_p_bar[key])\n",
    "        plt.barh(range(n_classes), fp_sorted, align='center', color='crimson', label='False Positive')\n",
    "        plt.barh(range(n_classes), tp_sorted, align='center', color='forestgreen', label='True Positive', left=fp_sorted)\n",
    "        # add legend\n",
    "        plt.legend(loc='lower right')\n",
    "        \"\"\"\n",
    "         Write number on side of bar\n",
    "        \"\"\"\n",
    "        fig = plt.gcf() # gcf - get current figure\n",
    "        axes = plt.gca()\n",
    "        r = fig.canvas.get_renderer()\n",
    "        for i, val in enumerate(sorted_values):\n",
    "            fp_val = fp_sorted[i]\n",
    "            tp_val = tp_sorted[i]\n",
    "            fp_str_val = \" \" + str(fp_val)\n",
    "            tp_str_val = fp_str_val + \" \" + str(tp_val)\n",
    "            # trick to paint multicolor with offset:\n",
    "            # first paint everything and then repaint the first number\n",
    "            t = plt.text(val, i, tp_str_val, color='forestgreen', va='center', fontweight='bold')\n",
    "            plt.text(val, i, fp_str_val, color='crimson', va='center', fontweight='bold')\n",
    "            if i == (len(sorted_values)-1): # largest bar\n",
    "                adjust_axes(r, t, fig, axes)\n",
    "    else:\n",
    "        plt.barh(range(n_classes), sorted_values, color=plot_color)\n",
    "        \"\"\"\n",
    "         Write number on side of bar\n",
    "        \"\"\"\n",
    "        fig = plt.gcf() # gcf - get current figure\n",
    "        axes = plt.gca()\n",
    "        r = fig.canvas.get_renderer()\n",
    "        for i, val in enumerate(sorted_values):\n",
    "            str_val = \" \" + str(val) # add a space before\n",
    "            if val < 1.0:\n",
    "                str_val = \" {0:.2f}\".format(val)\n",
    "            t = plt.text(val, i, str_val, color=plot_color, va='center', fontweight='bold')\n",
    "            # re-set axes to show number inside the figure\n",
    "            if i == (len(sorted_values)-1): # largest bar\n",
    "                adjust_axes(r, t, fig, axes)\n",
    "    # set window title\n",
    "    fig.canvas.set_window_title(window_title)\n",
    "    # write classes in y axis\n",
    "    tick_font_size = 12\n",
    "    plt.yticks(range(n_classes), sorted_keys, fontsize=tick_font_size)\n",
    "    \"\"\"\n",
    "     Re-scale height accordingly\n",
    "    \"\"\"\n",
    "    init_height = fig.get_figheight()\n",
    "    # comput the matrix height in points and inches\n",
    "    dpi = fig.dpi\n",
    "    height_pt = n_classes * (tick_font_size * 1.4) # 1.4 (some spacing)\n",
    "    height_in = height_pt / dpi\n",
    "    # compute the required figure height \n",
    "    top_margin = 0.15 # in percentage of the figure height\n",
    "    bottom_margin = 0.05 # in percentage of the figure height\n",
    "    figure_height = height_in / (1 - top_margin - bottom_margin)\n",
    "    # set new height\n",
    "    if figure_height > init_height:\n",
    "        fig.set_figheight(figure_height)\n",
    "\n",
    "    # set plot title\n",
    "    plt.title(plot_title, fontsize=14)\n",
    "    # set axis titles\n",
    "    # plt.xlabel('classes')\n",
    "    plt.xlabel(x_label, fontsize='large')\n",
    "    # adjust size of window\n",
    "    fig.tight_layout()\n",
    "    # save the plot\n",
    "    fig.savefig(output_path)\n",
    "    # show image\n",
    "    if to_show:\n",
    "        plt.show()\n",
    "    # close the plot\n",
    "    plt.close()\n",
    "\n",
    "\"\"\"\n",
    " Create a \".temp_files/\" and \"output/\" directory\n",
    "\"\"\"\n",
    "TEMP_FILES_PATH = \".temp_files\"\n",
    "if not os.path.exists(TEMP_FILES_PATH): # if it doesn't exist already\n",
    "    os.makedirs(TEMP_FILES_PATH)\n",
    "output_files_path = \"output\"\n",
    "if os.path.exists(output_files_path): # if it exist already\n",
    "    # reset the output directory\n",
    "    shutil.rmtree(output_files_path)\n",
    "\n",
    "os.makedirs(output_files_path)\n",
    "if draw_plot:\n",
    "    os.makedirs(os.path.join(output_files_path, \"classes\"))\n",
    "if show_animation:\n",
    "    os.makedirs(os.path.join(output_files_path, \"images\", \"detections_one_by_one\"))\n",
    "\n",
    "\"\"\"\n",
    " ground-truth\n",
    "     Load each of the ground-truth files into a temporary \".json\" file.\n",
    "     Create a list of all the class names present in the ground-truth (gt_classes).\n",
    "\"\"\"\n",
    "# get a list with the ground-truth files\n",
    "ground_truth_files_list = glob.glob(GT_PATH + '/*.txt')\n",
    "if len(ground_truth_files_list) == 0:\n",
    "    error(\"Error: No ground-truth files found!\")\n",
    "ground_truth_files_list.sort()\n",
    "# dictionary with counter per class\n",
    "gt_counter_per_class = {}\n",
    "counter_images_per_class = {}\n",
    "\n",
    "gt_files = []\n",
    "for txt_file in ground_truth_files_list:\n",
    "    #print(txt_file)\n",
    "    file_id = txt_file.split(\".txt\", 1)[0]\n",
    "    file_id = os.path.basename(os.path.normpath(file_id))\n",
    "    # check if there is a correspondent detection-results file\n",
    "    temp_path = os.path.join(DR_PATH, (file_id + \".txt\"))\n",
    "    if not os.path.exists(temp_path):\n",
    "        error_msg = \"Error. File not found: {}\\n\".format(temp_path)\n",
    "        error_msg += \"(You can avoid this error message by running extra/intersect-gt-and-dr.py)\"\n",
    "        error(error_msg)\n",
    "    lines_list = file_lines_to_list(txt_file)\n",
    "    # create ground-truth dictionary\n",
    "    bounding_boxes = []\n",
    "    is_difficult = False\n",
    "    already_seen_classes = []\n",
    "    for line in lines_list:\n",
    "        try:\n",
    "            if \"difficult\" in line:\n",
    "                    class_name, left, top, right, bottom, _difficult = line.split()\n",
    "                    is_difficult = True\n",
    "            else:\n",
    "                    class_name, left, top, right, bottom = line.split()\n",
    "        except ValueError:\n",
    "            error_msg = \"Error: File \" + txt_file + \" in the wrong format.\\n\"\n",
    "            error_msg += \" Expected: <class_name> <left> <top> <right> <bottom> ['difficult']\\n\"\n",
    "            error_msg += \" Received: \" + line\n",
    "            error_msg += \"\\n\\nIf you have a <class_name> with spaces between words you should remove them\\n\"\n",
    "            error_msg += \"by running the script \\\"remove_space.py\\\" or \\\"rename_class.py\\\" in the \\\"extra/\\\" folder.\"\n",
    "            error(error_msg)\n",
    "        # check if class is in the ignore list, if yes skip\n",
    "        if class_name in args.ignore:\n",
    "            continue\n",
    "        bbox = left + \" \" + top + \" \" + right + \" \" +bottom\n",
    "        if is_difficult:\n",
    "            bounding_boxes.append({\"class_name\":class_name, \"bbox\":bbox, \"used\":False, \"difficult\":True})\n",
    "            is_difficult = False\n",
    "        else:\n",
    "            bounding_boxes.append({\"class_name\":class_name, \"bbox\":bbox, \"used\":False})\n",
    "            # count that object\n",
    "            if class_name in gt_counter_per_class:\n",
    "                gt_counter_per_class[class_name] += 1\n",
    "            else:\n",
    "                # if class didn't exist yet\n",
    "                gt_counter_per_class[class_name] = 1\n",
    "\n",
    "            if class_name not in already_seen_classes:\n",
    "                if class_name in counter_images_per_class:\n",
    "                    counter_images_per_class[class_name] += 1\n",
    "                else:\n",
    "                    # if class didn't exist yet\n",
    "                    counter_images_per_class[class_name] = 1\n",
    "                already_seen_classes.append(class_name)\n",
    "\n",
    "\n",
    "    # dump bounding_boxes into a \".json\" file\n",
    "    new_temp_file = TEMP_FILES_PATH + \"/\" + file_id + \"_ground_truth.json\"\n",
    "    gt_files.append(new_temp_file)\n",
    "    with open(new_temp_file, 'w') as outfile:\n",
    "        json.dump(bounding_boxes, outfile)\n",
    "\n",
    "gt_classes = list(gt_counter_per_class.keys())\n",
    "# let's sort the classes alphabetically\n",
    "gt_classes = sorted(gt_classes)\n",
    "n_classes = len(gt_classes)\n",
    "#print(gt_classes)\n",
    "#print(gt_counter_per_class)\n",
    "\n",
    "\"\"\"\n",
    " Check format of the flag --set-class-iou (if used)\n",
    "    e.g. check if class exists\n",
    "\"\"\"\n",
    "if specific_iou_flagged:\n",
    "    n_args = len(args.set_class_iou)\n",
    "    error_msg = \\\n",
    "        '\\n --set-class-iou [class_1] [IoU_1] [class_2] [IoU_2] [...]'\n",
    "    if n_args % 2 != 0:\n",
    "        error('Error, missing arguments. Flag usage:' + error_msg)\n",
    "    # [class_1] [IoU_1] [class_2] [IoU_2]\n",
    "    # specific_iou_classes = ['class_1', 'class_2']\n",
    "    specific_iou_classes = args.set_class_iou[::2] # even\n",
    "    # iou_list = ['IoU_1', 'IoU_2']\n",
    "    iou_list = args.set_class_iou[1::2] # odd\n",
    "    if len(specific_iou_classes) != len(iou_list):\n",
    "        error('Error, missing arguments. Flag usage:' + error_msg)\n",
    "    for tmp_class in specific_iou_classes:\n",
    "        if tmp_class not in gt_classes:\n",
    "                    error('Error, unknown class \\\"' + tmp_class + '\\\". Flag usage:' + error_msg)\n",
    "    for num in iou_list:\n",
    "        if not is_float_between_0_and_1(num):\n",
    "            error('Error, IoU must be between 0.0 and 1.0. Flag usage:' + error_msg)\n",
    "\n",
    "\"\"\"\n",
    " detection-results\n",
    "     Load each of the detection-results files into a temporary \".json\" file.\n",
    "\"\"\"\n",
    "# get a list with the detection-results files\n",
    "dr_files_list = glob.glob(DR_PATH + '/*.txt')\n",
    "dr_files_list.sort()\n",
    "\n",
    "for class_index, class_name in enumerate(gt_classes):\n",
    "    bounding_boxes = []\n",
    "    for txt_file in dr_files_list:\n",
    "        #print(txt_file)\n",
    "        # the first time it checks if all the corresponding ground-truth files exist\n",
    "        file_id = txt_file.split(\".txt\",1)[0]\n",
    "        file_id = os.path.basename(os.path.normpath(file_id))\n",
    "        temp_path = os.path.join(GT_PATH, (file_id + \".txt\"))\n",
    "        if class_index == 0:\n",
    "            if not os.path.exists(temp_path):\n",
    "                error_msg = \"Error. File not found: {}\\n\".format(temp_path)\n",
    "                error_msg += \"(You can avoid this error message by running extra/intersect-gt-and-dr.py)\"\n",
    "                error(error_msg)\n",
    "        lines = file_lines_to_list(txt_file)\n",
    "        for line in lines:\n",
    "            try:\n",
    "                tmp_class_name, confidence, left, top, right, bottom = line.split()\n",
    "            except ValueError:\n",
    "                error_msg = \"Error: File \" + txt_file + \" in the wrong format.\\n\"\n",
    "                error_msg += \" Expected: <class_name> <confidence> <left> <top> <right> <bottom>\\n\"\n",
    "                error_msg += \" Received: \" + line\n",
    "                error(error_msg)\n",
    "            if tmp_class_name == class_name:\n",
    "                #print(\"match\")\n",
    "                bbox = left + \" \" + top + \" \" + right + \" \" +bottom\n",
    "                bounding_boxes.append({\"confidence\":confidence, \"file_id\":file_id, \"bbox\":bbox})\n",
    "                #print(bounding_boxes)\n",
    "    # sort detection-results by decreasing confidence\n",
    "    bounding_boxes.sort(key=lambda x:float(x['confidence']), reverse=True)\n",
    "    with open(TEMP_FILES_PATH + \"/\" + class_name + \"_dr.json\", 'w') as outfile:\n",
    "        json.dump(bounding_boxes, outfile)\n",
    "\n",
    "\"\"\"\n",
    " Calculate the AP for each class\n",
    "\"\"\"\n",
    "sum_AP = 0.0\n",
    "ap_dictionary = {}\n",
    "lamr_dictionary = {}\n",
    "# open file to store the output\n",
    "with open(output_files_path + \"/output.txt\", 'w') as output_file:\n",
    "    output_file.write(\"# AP and precision/recall per class\\n\")\n",
    "    count_true_positives = {}\n",
    "    for class_index, class_name in enumerate(gt_classes):\n",
    "        count_true_positives[class_name] = 0\n",
    "        \"\"\"\n",
    "         Load detection-results of that class\n",
    "        \"\"\"\n",
    "        dr_file = TEMP_FILES_PATH + \"/\" + class_name + \"_dr.json\"\n",
    "        dr_data = json.load(open(dr_file))\n",
    "\n",
    "        \"\"\"\n",
    "         Assign detection-results to ground-truth objects\n",
    "        \"\"\"\n",
    "        nd = len(dr_data)\n",
    "        tp = [0] * nd # creates an array of zeros of size nd\n",
    "        fp = [0] * nd\n",
    "        for idx, detection in enumerate(dr_data):\n",
    "            file_id = detection[\"file_id\"]\n",
    "            if show_animation:\n",
    "                # find ground truth image\n",
    "                ground_truth_img = glob.glob1(IMG_PATH, file_id + \".*\")\n",
    "                #tifCounter = len(glob.glob1(myPath,\"*.tif\"))\n",
    "                if len(ground_truth_img) == 0:\n",
    "                    error(\"Error. Image not found with id: \" + file_id)\n",
    "                elif len(ground_truth_img) > 1:\n",
    "                    error(\"Error. Multiple image with id: \" + file_id)\n",
    "                else: # found image\n",
    "                    #print(IMG_PATH + \"/\" + ground_truth_img[0])\n",
    "                    # Load image\n",
    "                    img = cv2.imread(IMG_PATH + \"/\" + ground_truth_img[0])\n",
    "                    # load image with draws of multiple detections\n",
    "                    img_cumulative_path = output_files_path + \"/images/\" + ground_truth_img[0]\n",
    "                    if os.path.isfile(img_cumulative_path):\n",
    "                        img_cumulative = cv2.imread(img_cumulative_path)\n",
    "                    else:\n",
    "                        img_cumulative = img.copy()\n",
    "                    # Add bottom border to image\n",
    "                    bottom_border = 60\n",
    "                    BLACK = [0, 0, 0]\n",
    "                    img = cv2.copyMakeBorder(img, 0, bottom_border, 0, 0, cv2.BORDER_CONSTANT, value=BLACK)\n",
    "            # assign detection-results to ground truth object if any\n",
    "            # open ground-truth with that file_id\n",
    "            gt_file = TEMP_FILES_PATH + \"/\" + file_id + \"_ground_truth.json\"\n",
    "            ground_truth_data = json.load(open(gt_file))\n",
    "            ovmax = -1\n",
    "            gt_match = -1\n",
    "            # load detected object bounding-box\n",
    "            bb = [ float(x) for x in detection[\"bbox\"].split() ]\n",
    "            for obj in ground_truth_data:\n",
    "                # look for a class_name match\n",
    "                if obj[\"class_name\"] == class_name:\n",
    "                    bbgt = [ float(x) for x in obj[\"bbox\"].split() ]\n",
    "                    bi = [max(bb[0],bbgt[0]), max(bb[1],bbgt[1]), min(bb[2],bbgt[2]), min(bb[3],bbgt[3])]\n",
    "                    iw = bi[2] - bi[0] + 1\n",
    "                    ih = bi[3] - bi[1] + 1\n",
    "                    if iw > 0 and ih > 0:\n",
    "                        # compute overlap (IoU) = area of intersection / area of union\n",
    "                        ua = (bb[2] - bb[0] + 1) * (bb[3] - bb[1] + 1) + (bbgt[2] - bbgt[0]\n",
    "                                        + 1) * (bbgt[3] - bbgt[1] + 1) - iw * ih\n",
    "                        ov = iw * ih / ua\n",
    "                        if ov > ovmax:\n",
    "                            ovmax = ov\n",
    "                            gt_match = obj\n",
    "\n",
    "            # assign detection as true positive/don't care/false positive\n",
    "            if show_animation:\n",
    "                status = \"NO MATCH FOUND!\" # status is only used in the animation\n",
    "            # set minimum overlap\n",
    "            min_overlap = MINOVERLAP\n",
    "            if specific_iou_flagged:\n",
    "                if class_name in specific_iou_classes:\n",
    "                    index = specific_iou_classes.index(class_name)\n",
    "                    min_overlap = float(iou_list[index])\n",
    "            if ovmax >= min_overlap:\n",
    "                if \"difficult\" not in gt_match:\n",
    "                        if not bool(gt_match[\"used\"]):\n",
    "                            # true positive\n",
    "                            tp[idx] = 1\n",
    "                            gt_match[\"used\"] = True\n",
    "                            count_true_positives[class_name] += 1\n",
    "                            # update the \".json\" file\n",
    "                            with open(gt_file, 'w') as f:\n",
    "                                    f.write(json.dumps(ground_truth_data))\n",
    "                            if show_animation:\n",
    "                                status = \"MATCH!\"\n",
    "                        else:\n",
    "                            # false positive (multiple detection)\n",
    "                            fp[idx] = 1\n",
    "                            if show_animation:\n",
    "                                status = \"REPEATED MATCH!\"\n",
    "            else:\n",
    "                # false positive\n",
    "                fp[idx] = 1\n",
    "                if ovmax > 0:\n",
    "                    status = \"INSUFFICIENT OVERLAP\"\n",
    "\n",
    "            \"\"\"\n",
    "             Draw image to show animation\n",
    "            \"\"\"\n",
    "            if show_animation:\n",
    "                height, widht = img.shape[:2]\n",
    "                # colors (OpenCV works with BGR)\n",
    "                white = (255,255,255)\n",
    "                light_blue = (255,200,100)\n",
    "                green = (0,255,0)\n",
    "                light_red = (30,30,255)\n",
    "                # 1st line\n",
    "                margin = 10\n",
    "                v_pos = int(height - margin - (bottom_border / 2.0))\n",
    "                text = \"Image: \" + ground_truth_img[0] + \" \"\n",
    "                img, line_width = draw_text_in_image(img, text, (margin, v_pos), white, 0)\n",
    "                text = \"Class [\" + str(class_index) + \"/\" + str(n_classes) + \"]: \" + class_name + \" \"\n",
    "                img, line_width = draw_text_in_image(img, text, (margin + line_width, v_pos), light_blue, line_width)\n",
    "                if ovmax != -1:\n",
    "                    color = light_red\n",
    "                    if status == \"INSUFFICIENT OVERLAP\":\n",
    "                        text = \"IoU: {0:.2f}% \".format(ovmax*100) + \"< {0:.2f}% \".format(min_overlap*100)\n",
    "                    else:\n",
    "                        text = \"IoU: {0:.2f}% \".format(ovmax*100) + \">= {0:.2f}% \".format(min_overlap*100)\n",
    "                        color = green\n",
    "                    img, _ = draw_text_in_image(img, text, (margin + line_width, v_pos), color, line_width)\n",
    "                # 2nd line\n",
    "                v_pos += int(bottom_border / 2.0)\n",
    "                rank_pos = str(idx+1) # rank position (idx starts at 0)\n",
    "                text = \"Detection #rank: \" + rank_pos + \" confidence: {0:.2f}% \".format(float(detection[\"confidence\"])*100)\n",
    "                img, line_width = draw_text_in_image(img, text, (margin, v_pos), white, 0)\n",
    "                color = light_red\n",
    "                if status == \"MATCH!\":\n",
    "                    color = green\n",
    "                text = \"Result: \" + status + \" \"\n",
    "                img, line_width = draw_text_in_image(img, text, (margin + line_width, v_pos), color, line_width)\n",
    "\n",
    "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                if ovmax > 0: # if there is intersections between the bounding-boxes\n",
    "                    bbgt = [ int(round(float(x))) for x in gt_match[\"bbox\"].split() ]\n",
    "                    cv2.rectangle(img,(bbgt[0],bbgt[1]),(bbgt[2],bbgt[3]),light_blue,2)\n",
    "                    cv2.rectangle(img_cumulative,(bbgt[0],bbgt[1]),(bbgt[2],bbgt[3]),light_blue,2)\n",
    "                    cv2.putText(img_cumulative, class_name, (bbgt[0],bbgt[1] - 5), font, 0.6, light_blue, 1, cv2.LINE_AA)\n",
    "                bb = [int(i) for i in bb]\n",
    "                cv2.rectangle(img,(bb[0],bb[1]),(bb[2],bb[3]),color,2)\n",
    "                cv2.rectangle(img_cumulative,(bb[0],bb[1]),(bb[2],bb[3]),color,2)\n",
    "                cv2.putText(img_cumulative, class_name, (bb[0],bb[1] - 5), font, 0.6, color, 1, cv2.LINE_AA)\n",
    "                # show image\n",
    "                cv2.imshow(\"Animation\", img)\n",
    "                cv2.waitKey(20) # show for 20 ms\n",
    "                # save image to output\n",
    "                output_img_path = output_files_path + \"/images/detections_one_by_one/\" + class_name + \"_detection\" + str(idx) + \".jpg\"\n",
    "                cv2.imwrite(output_img_path, img)\n",
    "                # save the image with all the objects drawn to it\n",
    "                cv2.imwrite(img_cumulative_path, img_cumulative)\n",
    "\n",
    "        #print(tp)\n",
    "        # compute precision/recall\n",
    "        cumsum = 0\n",
    "        for idx, val in enumerate(fp):\n",
    "            fp[idx] += cumsum\n",
    "            cumsum += val\n",
    "        cumsum = 0\n",
    "        for idx, val in enumerate(tp):\n",
    "            tp[idx] += cumsum\n",
    "            cumsum += val\n",
    "        #print(tp)\n",
    "        rec = tp[:]\n",
    "        for idx, val in enumerate(tp):\n",
    "            rec[idx] = float(tp[idx]) / gt_counter_per_class[class_name]\n",
    "        #print(rec)\n",
    "        prec = tp[:]\n",
    "        for idx, val in enumerate(tp):\n",
    "            prec[idx] = float(tp[idx]) / (fp[idx] + tp[idx])\n",
    "        #print(prec)\n",
    "\n",
    "        ap, mrec, mprec = voc_ap(rec[:], prec[:])\n",
    "        sum_AP += ap\n",
    "        text = \"{0:.2f}%\".format(ap*100) + \" = \" + class_name + \" AP \" #class_name + \" AP = {0:.2f}%\".format(ap*100)\n",
    "        \"\"\"\n",
    "         Write to output.txt\n",
    "        \"\"\"\n",
    "        rounded_prec = [ '%.2f' % elem for elem in prec ]\n",
    "        rounded_rec = [ '%.2f' % elem for elem in rec ]\n",
    "        output_file.write(text + \"\\n Precision: \" + str(rounded_prec) + \"\\n Recall :\" + str(rounded_rec) + \"\\n\\n\")\n",
    "        if not args.quiet:\n",
    "            print(text)\n",
    "        ap_dictionary[class_name] = ap\n",
    "\n",
    "        n_images = counter_images_per_class[class_name]\n",
    "        lamr, mr, fppi = log_average_miss_rate(np.array(prec), np.array(rec), n_images)\n",
    "        lamr_dictionary[class_name] = lamr\n",
    "\n",
    "        \"\"\"\n",
    "         Draw plot\n",
    "        \"\"\"\n",
    "        if draw_plot:\n",
    "            plt.plot(rec, prec, '-o')\n",
    "            # add a new penultimate point to the list (mrec[-2], 0.0)\n",
    "            # since the last line segment (and respective area) do not affect the AP value\n",
    "            area_under_curve_x = mrec[:-1] + [mrec[-2]] + [mrec[-1]]\n",
    "            area_under_curve_y = mprec[:-1] + [0.0] + [mprec[-1]]\n",
    "            plt.fill_between(area_under_curve_x, 0, area_under_curve_y, alpha=0.2, edgecolor='r')\n",
    "            # set window title\n",
    "            fig = plt.gcf() # gcf - get current figure\n",
    "            fig.canvas.set_window_title('AP ' + class_name)\n",
    "            # set plot title\n",
    "            plt.title('class: ' + text)\n",
    "            #plt.suptitle('This is a somewhat long figure title', fontsize=16)\n",
    "            # set axis titles\n",
    "            plt.xlabel('Recall')\n",
    "            plt.ylabel('Precision')\n",
    "            # optional - set axes\n",
    "            axes = plt.gca() # gca - get current axes\n",
    "            axes.set_xlim([0.0,1.0])\n",
    "            axes.set_ylim([0.0,1.05]) # .05 to give some extra space\n",
    "            # Alternative option -> wait for button to be pressed\n",
    "            #while not plt.waitforbuttonpress(): pass # wait for key display\n",
    "            # Alternative option -> normal display\n",
    "            #plt.show()\n",
    "            # save the plot\n",
    "            fig.savefig(output_files_path + \"/classes/\" + class_name + \".png\")\n",
    "            plt.cla() # clear axes for next plot\n",
    "\n",
    "    if show_animation:\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    output_file.write(\"\\n# mAP of all classes\\n\")\n",
    "    mAP = sum_AP / n_classes\n",
    "    text = \"mAP = {0:.2f}%\".format(mAP*100)\n",
    "    output_file.write(text + \"\\n\")\n",
    "    print(text)\n",
    "\n",
    "\"\"\"\n",
    " Draw false negatives\n",
    "\"\"\"\n",
    "if show_animation:\n",
    "    pink = (203,192,255)\n",
    "    for tmp_file in gt_files:\n",
    "        ground_truth_data = json.load(open(tmp_file))\n",
    "        #print(ground_truth_data)\n",
    "        # get name of corresponding image\n",
    "        start = TEMP_FILES_PATH + '/'\n",
    "        img_id = tmp_file[tmp_file.find(start)+len(start):tmp_file.rfind('_ground_truth.json')]\n",
    "        img_cumulative_path = output_files_path + \"/images/\" + img_id + \".jpg\"\n",
    "        img = cv2.imread(img_cumulative_path)\n",
    "        if img is None:\n",
    "            img_path = IMG_PATH + '/' + img_id + \".jpg\"\n",
    "            img = cv2.imread(img_path)\n",
    "        # draw false negatives\n",
    "        for obj in ground_truth_data:\n",
    "            if not obj['used']:\n",
    "                bbgt = [ int(round(float(x))) for x in obj[\"bbox\"].split() ]\n",
    "                cv2.rectangle(img,(bbgt[0],bbgt[1]),(bbgt[2],bbgt[3]),pink,2)\n",
    "        cv2.imwrite(img_cumulative_path, img)\n",
    "\n",
    "# remove the temp_files directory\n",
    "shutil.rmtree(TEMP_FILES_PATH)\n",
    "\n",
    "\"\"\"\n",
    " Count total of detection-results\n",
    "\"\"\"\n",
    "# iterate through all the files\n",
    "det_counter_per_class = {}\n",
    "for txt_file in dr_files_list:\n",
    "    # get lines to list\n",
    "    lines_list = file_lines_to_list(txt_file)\n",
    "    for line in lines_list:\n",
    "        class_name = line.split()[0]\n",
    "        # check if class is in the ignore list, if yes skip\n",
    "        if class_name in args.ignore:\n",
    "            continue\n",
    "        # count that object\n",
    "        if class_name in det_counter_per_class:\n",
    "            det_counter_per_class[class_name] += 1\n",
    "        else:\n",
    "            # if class didn't exist yet\n",
    "            det_counter_per_class[class_name] = 1\n",
    "#print(det_counter_per_class)\n",
    "dr_classes = list(det_counter_per_class.keys())\n",
    "\n",
    "\n",
    "\"\"\"\n",
    " Plot the total number of occurences of each class in the ground-truth\n",
    "\"\"\"\n",
    "if draw_plot:\n",
    "    window_title = \"ground-truth-info\"\n",
    "    plot_title = \"ground-truth\\n\"\n",
    "    plot_title += \"(\" + str(len(ground_truth_files_list)) + \" files and \" + str(n_classes) + \" classes)\"\n",
    "    x_label = \"Number of objects per class\"\n",
    "    output_path = output_files_path + \"/ground-truth-info.png\"\n",
    "    to_show = False\n",
    "    plot_color = 'forestgreen'\n",
    "    draw_plot_func(\n",
    "        gt_counter_per_class,\n",
    "        n_classes,\n",
    "        window_title,\n",
    "        plot_title,\n",
    "        x_label,\n",
    "        output_path,\n",
    "        to_show,\n",
    "        plot_color,\n",
    "        '',\n",
    "        )\n",
    "\n",
    "\"\"\"\n",
    " Write number of ground-truth objects per class to results.txt\n",
    "\"\"\"\n",
    "with open(output_files_path + \"/output.txt\", 'a') as output_file:\n",
    "    output_file.write(\"\\n# Number of ground-truth objects per class\\n\")\n",
    "    for class_name in sorted(gt_counter_per_class):\n",
    "        output_file.write(class_name + \": \" + str(gt_counter_per_class[class_name]) + \"\\n\")\n",
    "\n",
    "\"\"\"\n",
    " Finish counting true positives\n",
    "\"\"\"\n",
    "for class_name in dr_classes:\n",
    "    # if class exists in detection-result but not in ground-truth then there are no true positives in that class\n",
    "    if class_name not in gt_classes:\n",
    "        count_true_positives[class_name] = 0\n",
    "#print(count_true_positives)\n",
    "\n",
    "\"\"\"\n",
    " Plot the total number of occurences of each class in the \"detection-results\" folder\n",
    "\"\"\"\n",
    "if draw_plot:\n",
    "    window_title = \"detection-results-info\"\n",
    "    # Plot title\n",
    "    plot_title = \"detection-results\\n\"\n",
    "    plot_title += \"(\" + str(len(dr_files_list)) + \" files and \"\n",
    "    count_non_zero_values_in_dictionary = sum(int(x) > 0 for x in list(det_counter_per_class.values()))\n",
    "    plot_title += str(count_non_zero_values_in_dictionary) + \" detected classes)\"\n",
    "    # end Plot title\n",
    "    x_label = \"Number of objects per class\"\n",
    "    output_path = output_files_path + \"/detection-results-info.png\"\n",
    "    to_show = False\n",
    "    plot_color = 'forestgreen'\n",
    "    true_p_bar = count_true_positives\n",
    "    draw_plot_func(\n",
    "        det_counter_per_class,\n",
    "        len(det_counter_per_class),\n",
    "        window_title,\n",
    "        plot_title,\n",
    "        x_label,\n",
    "        output_path,\n",
    "        to_show,\n",
    "        plot_color,\n",
    "        true_p_bar\n",
    "        )\n",
    "\n",
    "\"\"\"\n",
    " Write number of detected objects per class to output.txt\n",
    "\"\"\"\n",
    "with open(output_files_path + \"/output.txt\", 'a') as output_file:\n",
    "    output_file.write(\"\\n# Number of detected objects per class\\n\")\n",
    "    for class_name in sorted(dr_classes):\n",
    "        n_det = det_counter_per_class[class_name]\n",
    "        text = class_name + \": \" + str(n_det)\n",
    "        text += \" (tp:\" + str(count_true_positives[class_name]) + \"\"\n",
    "        text += \", fp:\" + str(n_det - count_true_positives[class_name]) + \")\\n\"\n",
    "        output_file.write(text)\n",
    "\n",
    "\"\"\"\n",
    " Draw log-average miss rate plot (Show lamr of all classes in decreasing order)\n",
    "\"\"\"\n",
    "if draw_plot:\n",
    "    window_title = \"lamr\"\n",
    "    plot_title = \"log-average miss rate\"\n",
    "    x_label = \"log-average miss rate\"\n",
    "    output_path = output_files_path + \"/lamr.png\"\n",
    "    to_show = False\n",
    "    plot_color = 'royalblue'\n",
    "    draw_plot_func(\n",
    "        lamr_dictionary,\n",
    "        n_classes,\n",
    "        window_title,\n",
    "        plot_title,\n",
    "        x_label,\n",
    "        output_path,\n",
    "        to_show,\n",
    "        plot_color,\n",
    "        \"\"\n",
    "        )\n",
    "\n",
    "\"\"\"\n",
    " Draw mAP plot (Show AP's of all classes in decreasing order)\n",
    "\"\"\"\n",
    "if draw_plot:\n",
    "    window_title = \"mAP\"\n",
    "    plot_title = \"mAP = {0:.2f}%\".format(mAP*100)\n",
    "    x_label = \"Average Precision\"\n",
    "    output_path = output_files_path + \"/mAP.png\"\n",
    "    to_show = True\n",
    "    plot_color = 'royalblue'\n",
    "    draw_plot_func(\n",
    "        ap_dictionary,\n",
    "        n_classes,\n",
    "        window_title,\n",
    "        plot_title,\n",
    "        x_label,\n",
    "        output_path,\n",
    "        to_show,\n",
    "        plot_color,\n",
    "        \"\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
